# Global configuration for GPU resource type
global:
  accelerator:
    # Supported values: nvidia.com/gpu, habana.ai/gaudi, amd.com/gpu
    resourceType: nvidia.com/gpu

servingRuntime:
  image: quay.io/ecosystem-appeng/vllm:openai-v0.8.5
  recommendedAccelerators:
    - "{{ .Values.global.accelerator.resourceType }}"

models:
  llama-3-2-1b-instruct:
    id: meta-llama/Llama-3.2-1B-Instruct
    enabled: false
    inferenceService:
      args:
        - --enable-auto-tool-choice
        - --chat-template
        - /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja
        - --tool-call-parser
        - llama3_json
        - --max-model-len
        - "30544"

  llama-guard-3-1b:
    id: meta-llama/Llama-Guard-3-1B
    enabled: false
    inferenceService:
      args:
        - --max-model-len
        - "14336"

  llama-3-2-3b-instruct:
    id: meta-llama/Llama-3.2-3B-Instruct
    enabled: false
    inferenceService:
      args:
        - --enable-auto-tool-choice
        - --chat-template
        - /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja
        - --tool-call-parser
        - llama3_json
        - --max-model-len
        - "30544"

  llama-guard-3-8b:
    id: meta-llama/Llama-Guard-3-8B
    enabled: false
    inferenceService:
      args:
        - --max-model-len
        - "14336"

  llama-3-1-8b-instruct:
    id: meta-llama/Llama-3.1-8B-Instruct
    enabled: false
    inferenceService:
      resources:
        limits:
          "{{ .Values.global.accelerator.resourceType }}": "1"
      args:
        - --max-model-len
        - "14336"
        - --enable-auto-tool-choice
        - --chat-template
        - /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja
        - --tool-call-parser
        - llama3_json

  llama-3-3-70b-instruct:
    id: meta-llama/Llama-3.3-70B-Instruct
    enabled: false
    storageSize: 150Gi
    inferenceService:
      resources:
        limits:
          "{{ .Values.global.accelerator.resourceType }}": "4"
      args:
        - --tensor-parallel-size
        - "4"
        - --gpu-memory-utilization
        - "0.95"
        - --quantization
        - fp8
        - --max-num-batched-tokens
        - "4096"
        - --enable-auto-tool-choice
        - --chat-template
        - /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja
        - --tool-call-parser
        - llama3_json
        - --swap-space
        - "32"

  llama-3-2-1b-instruct-quantized:
    id: RedHatAI/Llama-3.2-1B-Instruct-quantized.w8a8
    enabled: false
    inferenceService:
      args:
        - --gpu-memory-utilization
        - "0.4"
        - --quantization
        - compressed-tensors
        - --enable-auto-tool-choice
        - --chat-template
        - /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja
        - --tool-call-parser
        - llama3_json
        - --max-model-len
        - "30544"
